{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# Lab assignment: analyzing movie reviews with Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "<img src=\"img/cinemaReviews.png\" style=\"width:600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "In this assignment we will analyze the sentiment, positive or negative, expressed in a set of movie reviews IMDB. To do so we will make use of word embeddings and recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "## Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "Throughout this notebook you will find empty cells that you will need to fill with your own code. Follow the instructions in the notebook and pay special attention to the following symbols.\n",
    "\n",
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>You will need to solve a question by writing your own code or answer in the cell immediately below, or in a different file as instructed.</td></tr>\n",
    " <tr><td><img src=\"img/exclamation.png\" style=\"width:80px;height:80px;\"></td><td>This is a hint or useful observation that can help you solve this assignment. You are not expected to write any solution, but you should pay attention to them to understand the assignment.</td></tr>\n",
    " <tr><td><img src=\"img/pro.png\" style=\"width:80px;height:80px;\"></td><td>This is an advanced and voluntary exercise that can help you gain a deeper knowledge into the topic. Good luck!</td></tr>\n",
    "</table>\n",
    "\n",
    "During the assigment you will make use of several Python packages that might not be installed in your machine. If that is the case, you can install new Python packages with\n",
    "\n",
    "    conda install PACKAGENAME\n",
    "    \n",
    "if you are using Python Anaconda. Else you should use\n",
    "\n",
    "    pip install PACKAGENAME\n",
    "\n",
    "You will need the following packages for this particular assignment. Make sure they are available before proceeding:\n",
    "\n",
    "* **numpy**\n",
    "* **keras**\n",
    "* **matplotlib**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will embed any plots into the notebook instead of generating a new window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, if you need any help on the usage of a Python function you can place the writing cursor over its name and press Caps+Shift to produce a pop-out with related documentation. This will only work inside code cells. \n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Keras library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will make use of the <a href=http://keras.io/>keras</a> Deep Learning library for Python. This library allows building several kinds of shallow and deep networks, following either a sequential or a graph architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of a part of the IMDB database on movie reviews. IMDB rates movies with a score ranging 0-10, but for simplicity we will consider a dataset of good and bad reviews, where a review has been considered bad with a score smaller than 4, and good if it features a score larger than 7. The data is available under the *data* folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Load the data into two variables, a list **text** with each of the movie reviews and a list **y** of the class labels.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(2500, 2)\n"
     ]
    }
   ],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "ingesta = pd.read_csv(\"./data/data.csv\", sep=\"\\t\")\n",
    "print(type(ingesta))\n",
    "print(ingesta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I simply cant understand why all these relics ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Director Raoul Walsh was like the Michael Bay ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>It could have been a better film. It does drag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>It is very hard to rate this film. As entertai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I've read some terrible things about this film...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  I simply cant understand why all these relics ...\n",
       "1          1  Director Raoul Walsh was like the Michael Bay ...\n",
       "2          1  It could have been a better film. It does drag...\n",
       "3          1  It is very hard to rate this film. As entertai...\n",
       "4          1  I've read some terrible things about this film..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingesta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = ingesta['text']\n",
    "y = ingesta['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience in what follows we will also split the data into a training and test subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Split the list of texts into **texts_train** and **texts_test** lists, keeping 25% of the texts for test. Split in the same way the labels, obtaining lists **y_train** and **y_test**.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Casy\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "texts_train, texts_test, y_train, y_test = train_test_split(text, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "1875\n",
      "<class 'pandas.core.series.Series'>\n",
      "625\n"
     ]
    }
   ],
   "source": [
    "print(type(texts_train))\n",
    "print(len(texts_train))\n",
    "print(type(texts_test))\n",
    "print(len(texts_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't introduce text directly into the network, so we will have to tranform it to a vector representation. To do so, we will first **tokenize** the text into words (or tokens), and assign a unique identifier to each word found in the text. Doing this will allow us to perform the encoding. We can do this easily by making use of the **Tokenizer** class in keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Tokenizer offers convenient methods to split texts down to tokens. At construction time we need to supply the Tokenizer the maximum number of different words we are willing to represent. If out texts have greater word variety than this number, the least frequent words will be discarded. We will choose a number large enough for our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxwords = 1000\n",
    "tokenizer = Tokenizer(nb_words = maxwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to **fit** the Tokenizer to the training texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Find in the keras documentation the appropriate Tokenizer method to fit the tokenizer on a list of text, then use it to fit it on the training data.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "texts_train_tokens = tokenizer.fit_on_texts(texts_train)\n",
    "print(type(texts_train_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If done correctly, the following should show the number of times the tokenizer has found each word in the input texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'waving': 1,\n",
       " 'switching': 1,\n",
       " 'whatever': 54,\n",
       " 'drill': 1,\n",
       " 'compensated': 3,\n",
       " 'choreographed': 8,\n",
       " 'secrets': 9,\n",
       " 'genders': 1,\n",
       " 'cliché': 21,\n",
       " 'doers': 1,\n",
       " 'crude': 14,\n",
       " 'ambitions': 6,\n",
       " \"doesen't\": 1,\n",
       " 'model': 20,\n",
       " 'basicaly': 1,\n",
       " 'catdog': 1,\n",
       " \"'burning\": 1,\n",
       " \"sleeve's\": 2,\n",
       " 'motivated': 2,\n",
       " 'derivitive': 1,\n",
       " \"'bad\": 1,\n",
       " 'repugnant': 3,\n",
       " 'executives': 9,\n",
       " 'arrives': 19,\n",
       " 'integral': 1,\n",
       " 'bubble': 5,\n",
       " 'efficiency': 3,\n",
       " 'articles': 1,\n",
       " 'rabbit': 7,\n",
       " 'say\\x85': 1,\n",
       " 'mutated': 1,\n",
       " \"'beyond\": 2,\n",
       " \"'slight'\": 1,\n",
       " 'misanthropic': 1,\n",
       " 'ambush': 1,\n",
       " 'deliberately': 8,\n",
       " 'psycho': 27,\n",
       " 'wishes': 12,\n",
       " 'antidote': 1,\n",
       " 'imprezza': 1,\n",
       " 'quebecois': 1,\n",
       " 'bourgeoise': 1,\n",
       " 'christophe': 1,\n",
       " 'bewitching': 1,\n",
       " 'thurman': 2,\n",
       " 'lake': 20,\n",
       " 'natural': 41,\n",
       " 'saturates': 1,\n",
       " 'deny': 13,\n",
       " 'payments': 1,\n",
       " 'skinheads': 1,\n",
       " 'sullied': 1,\n",
       " 'viewpoint': 1,\n",
       " 'dinosaur': 5,\n",
       " 'pace': 42,\n",
       " 'stuntmen': 1,\n",
       " 'utilize': 2,\n",
       " 'specified': 1,\n",
       " 'archaic': 3,\n",
       " 'picky': 3,\n",
       " 'paraphrase': 1,\n",
       " 'gossiping': 1,\n",
       " 'breakdance': 1,\n",
       " 'unborn': 1,\n",
       " 'violent': 45,\n",
       " \"winter's\": 1,\n",
       " 'injustice': 4,\n",
       " 'policemen': 3,\n",
       " 'ons': 3,\n",
       " 'relativity': 1,\n",
       " 'earns': 2,\n",
       " 'stereotyping': 2,\n",
       " 'ming': 1,\n",
       " 'heroic': 6,\n",
       " 'colors': 11,\n",
       " 'waddles': 1,\n",
       " 'grateful': 6,\n",
       " 'slim': 6,\n",
       " 'charlotte': 11,\n",
       " 'activist': 2,\n",
       " 'kids': 140,\n",
       " 'medley': 1,\n",
       " \"editing'la\": 1,\n",
       " 'character': 509,\n",
       " 'ramps': 1,\n",
       " 'hallucinations': 3,\n",
       " 'improbable': 5,\n",
       " \"'lagaan'\": 1,\n",
       " 'riviting': 1,\n",
       " 'resembled': 2,\n",
       " 'csi': 3,\n",
       " 'f': 40,\n",
       " 'sappiness': 1,\n",
       " 'wee': 4,\n",
       " 'motorcycles': 1,\n",
       " 'bundle': 1,\n",
       " 'donut': 2,\n",
       " 'quick': 32,\n",
       " 'ukulele': 1,\n",
       " \"'unknown\": 1,\n",
       " 'donning': 1,\n",
       " 'shining': 7,\n",
       " 'logo': 4,\n",
       " 'compromised': 4,\n",
       " 'retellings': 1,\n",
       " 'innovator': 2,\n",
       " \"librarian's\": 1,\n",
       " \"boss'\": 1,\n",
       " 'cupboard': 2,\n",
       " 'trump': 4,\n",
       " 'needlepoint': 1,\n",
       " 'raider': 3,\n",
       " 'syringe': 1,\n",
       " 'caliber': 4,\n",
       " 'population': 6,\n",
       " 'deathtrap': 1,\n",
       " \"'new\": 1,\n",
       " 'trees': 5,\n",
       " 'vertigo': 3,\n",
       " 'unearp': 1,\n",
       " 'wonky': 1,\n",
       " 'periodicals': 1,\n",
       " 'erasing': 2,\n",
       " 'mesmerising': 2,\n",
       " 'cutest': 1,\n",
       " 'dreamer': 1,\n",
       " \"liang's\": 1,\n",
       " 'fable': 2,\n",
       " 'appreciation': 12,\n",
       " 'kriemhild': 4,\n",
       " 'bibbidy': 1,\n",
       " 'depicting': 8,\n",
       " 'barry': 8,\n",
       " \"fallon's\": 1,\n",
       " 'dominatrix': 2,\n",
       " 'challenged': 7,\n",
       " 'sades': 1,\n",
       " 'heinz': 2,\n",
       " 'intentionally': 4,\n",
       " \"streep's\": 6,\n",
       " \"1996's\": 1,\n",
       " 'troops': 5,\n",
       " 'chafes': 1,\n",
       " 'grapes': 3,\n",
       " \"borges'\": 1,\n",
       " 'bates': 2,\n",
       " 'secretary': 18,\n",
       " 'mady': 1,\n",
       " 'putting': 25,\n",
       " \"lost's\": 1,\n",
       " 'walkers': 4,\n",
       " 'strikes': 7,\n",
       " 'shortfalls': 1,\n",
       " 'installations': 1,\n",
       " \"giovanna's\": 1,\n",
       " 'argue': 11,\n",
       " 'nova': 1,\n",
       " 'brutes': 1,\n",
       " \"you's\": 1,\n",
       " 'sends': 8,\n",
       " 'obsolete': 5,\n",
       " 'enforcement': 4,\n",
       " 'financially': 1,\n",
       " 'peeking': 1,\n",
       " 'gut': 4,\n",
       " 'rochon': 1,\n",
       " 'bigfoot': 2,\n",
       " 'thoughtful': 5,\n",
       " 'mono': 3,\n",
       " 'forever': 37,\n",
       " 'unfeeling': 2,\n",
       " 'filing': 1,\n",
       " 'areas': 10,\n",
       " 'soul': 39,\n",
       " 'graceful': 3,\n",
       " 'aural': 2,\n",
       " 'superlative': 3,\n",
       " 'metschurat': 3,\n",
       " 'fiascos': 1,\n",
       " 'schwartz': 2,\n",
       " 'olsen': 2,\n",
       " \"robin'\": 1,\n",
       " 'gouge': 2,\n",
       " 'gungaroo': 1,\n",
       " 'series': 272,\n",
       " 'deliriously': 1,\n",
       " 'swindler': 1,\n",
       " 'bird': 11,\n",
       " 'cigliutti': 1,\n",
       " 'flopperoo': 1,\n",
       " 'chosen': 19,\n",
       " 'gentleness': 1,\n",
       " 'completely': 150,\n",
       " 'faulty': 1,\n",
       " 'oar': 1,\n",
       " 'riffraff': 1,\n",
       " 'navigable': 1,\n",
       " 'priced': 1,\n",
       " 'plagues': 2,\n",
       " \"cage's\": 2,\n",
       " 'touched': 16,\n",
       " 'monochrome': 1,\n",
       " 'evidenced': 2,\n",
       " 'acclaimed': 4,\n",
       " 'aunt': 10,\n",
       " 'ii': 22,\n",
       " 'sifu': 1,\n",
       " 'lotte': 1,\n",
       " 'creed': 1,\n",
       " 'combo': 2,\n",
       " 'consequently': 4,\n",
       " 'beverley': 1,\n",
       " 'guinness': 2,\n",
       " 'laughlin': 1,\n",
       " 'edible': 1,\n",
       " 'shawshank': 2,\n",
       " 'atrocities': 4,\n",
       " 'charlo': 1,\n",
       " 'store': 37,\n",
       " 'sleazy': 14,\n",
       " 'helena': 3,\n",
       " 'inspection': 2,\n",
       " 'dang': 1,\n",
       " 'constellation': 1,\n",
       " 'staging': 5,\n",
       " \"whoopie's\": 1,\n",
       " 'expletive': 2,\n",
       " 'dilemma': 2,\n",
       " 'attendance': 4,\n",
       " 'sonja': 3,\n",
       " 'highlighted': 2,\n",
       " 'slowmotion': 1,\n",
       " 'backing': 3,\n",
       " 'lamentably': 1,\n",
       " 'tucked': 1,\n",
       " 'human': 125,\n",
       " 'fueled': 1,\n",
       " 'assignment': 5,\n",
       " 'creamy': 1,\n",
       " 'underpinnings': 1,\n",
       " 'idiotic': 12,\n",
       " 'discs': 2,\n",
       " 'blanca': 1,\n",
       " 'bidding': 1,\n",
       " 'sale': 3,\n",
       " 'faustian': 1,\n",
       " \"baker's\": 3,\n",
       " 'alibi': 2,\n",
       " 'zane': 4,\n",
       " 'gosh': 2,\n",
       " 'blubbering': 1,\n",
       " \"filmmaker's\": 2,\n",
       " 'oedipus': 1,\n",
       " 'kris': 6,\n",
       " 'soothing': 1,\n",
       " 'readjusts': 1,\n",
       " 'bondage': 7,\n",
       " 'debunks': 1,\n",
       " 'dylan': 5,\n",
       " 'cottontail': 1,\n",
       " 'reel': 6,\n",
       " 'stranded': 9,\n",
       " 'concrete': 1,\n",
       " 'accountable': 1,\n",
       " 'pettiness': 1,\n",
       " 'mausoleum': 2,\n",
       " 'jlu': 1,\n",
       " 'contribute': 5,\n",
       " 'ladykillers': 1,\n",
       " 'welcomed': 1,\n",
       " 'forcing': 7,\n",
       " 'farcical': 2,\n",
       " 'endeavour': 1,\n",
       " 'admiring': 2,\n",
       " 'underwood': 2,\n",
       " 'matriarch': 2,\n",
       " 'portraits': 1,\n",
       " 'remotes': 1,\n",
       " 'filippines': 1,\n",
       " 'scolds': 1,\n",
       " 'bureaucracy': 2,\n",
       " \"'slow\": 1,\n",
       " 'nikita': 1,\n",
       " 'seaside': 3,\n",
       " 'revolutionised': 1,\n",
       " 'repeat': 13,\n",
       " 'el': 6,\n",
       " 'remorse': 2,\n",
       " 'flag': 2,\n",
       " 'kyser': 6,\n",
       " \"'see\": 1,\n",
       " 'bombard': 1,\n",
       " 'cineplex': 1,\n",
       " \"'jay\": 1,\n",
       " \"fi'\": 1,\n",
       " 'lange': 3,\n",
       " 'tetsuo': 1,\n",
       " 'competed': 1,\n",
       " 'yellows': 1,\n",
       " 'daisuke': 1,\n",
       " 'exploding': 7,\n",
       " 'gastaldi': 2,\n",
       " 'seducing': 1,\n",
       " 'bryden': 2,\n",
       " 'finality': 1,\n",
       " 'honestly': 28,\n",
       " 'calender': 1,\n",
       " 'recorded': 7,\n",
       " 'hasso': 2,\n",
       " 'substances': 2,\n",
       " 'elsa': 3,\n",
       " 'settlements': 1,\n",
       " 'sidewalks': 1,\n",
       " 'offbeat': 3,\n",
       " 'gripe': 1,\n",
       " 'villainously': 1,\n",
       " 'elaborating': 1,\n",
       " 'brio': 1,\n",
       " 'becce': 2,\n",
       " 'aniston': 2,\n",
       " 'minion': 1,\n",
       " 'occurs': 9,\n",
       " 'bunches': 1,\n",
       " 'giggolo': 1,\n",
       " 'screen': 180,\n",
       " 'scantily': 3,\n",
       " 'native': 15,\n",
       " 'tunnelvision': 1,\n",
       " 'patched': 1,\n",
       " 'uriah': 2,\n",
       " 'manson': 2,\n",
       " 'rife': 1,\n",
       " 'charybdis': 1,\n",
       " 'command': 4,\n",
       " 'superior': 29,\n",
       " 'edmonton': 1,\n",
       " 'deep': 51,\n",
       " 'baseman': 1,\n",
       " 'facing': 5,\n",
       " \"'dark\": 1,\n",
       " 'tau': 1,\n",
       " 'avaricious': 1,\n",
       " 'auditioning': 2,\n",
       " 'textbook': 1,\n",
       " 'inmates': 1,\n",
       " 'sword': 15,\n",
       " 'nosed': 2,\n",
       " 'fenech': 1,\n",
       " 'dimension': 10,\n",
       " 'creatively': 1,\n",
       " 'erosion': 1,\n",
       " 'ribbons': 2,\n",
       " 'ckin': 1,\n",
       " 'genuinely': 21,\n",
       " 'trace': 10,\n",
       " 'rubbish': 26,\n",
       " '132': 1,\n",
       " 'ex': 35,\n",
       " 'unreleased': 2,\n",
       " 'unmemorably': 1,\n",
       " '1998': 3,\n",
       " 'sillier': 2,\n",
       " \"original's\": 1,\n",
       " 'marseilles': 1,\n",
       " 'creatures': 13,\n",
       " 'childhood': 22,\n",
       " \"'alice'\": 1,\n",
       " '“golden': 2,\n",
       " 'homely': 3,\n",
       " 'naschy': 1,\n",
       " 'oblige': 1,\n",
       " 'role': 210,\n",
       " \"spielberg's\": 4,\n",
       " 'capitalism': 3,\n",
       " 'replacement': 1,\n",
       " 'carter': 14,\n",
       " 'means': 55,\n",
       " 'moonwalker': 2,\n",
       " 'phantasm': 10,\n",
       " 'ashamed': 7,\n",
       " 'saratoga': 1,\n",
       " 'appalling': 3,\n",
       " 'cesar': 2,\n",
       " 'overlook': 9,\n",
       " 'curse': 16,\n",
       " 'clouds': 3,\n",
       " 'mounting': 1,\n",
       " 'flapjack': 1,\n",
       " 'convalesces': 1,\n",
       " 'fetched': 9,\n",
       " 'execute': 1,\n",
       " 'ruin': 8,\n",
       " 'bastardized': 1,\n",
       " 'audie': 1,\n",
       " 'filmographies': 3,\n",
       " \"price'\": 1,\n",
       " '7ft': 1,\n",
       " 'bargains': 1,\n",
       " 'kranks': 1,\n",
       " 'keyhole': 1,\n",
       " 'remedy': 1,\n",
       " 'kagan': 1,\n",
       " 'online': 6,\n",
       " 'under': 103,\n",
       " 'gussied': 1,\n",
       " 'prepon': 1,\n",
       " 'adolescence': 3,\n",
       " 'ksm': 1,\n",
       " 'cady': 1,\n",
       " 'contaminated': 1,\n",
       " 'cavanaugh': 1,\n",
       " 'assess': 1,\n",
       " 'slurred': 1,\n",
       " 'sirens': 1,\n",
       " 'floor': 20,\n",
       " '195': 1,\n",
       " 'curtains': 3,\n",
       " 'madsen': 1,\n",
       " \"ae's\": 1,\n",
       " 'evidently': 6,\n",
       " '1100': 1,\n",
       " 'fave': 1,\n",
       " \"hutton's\": 1,\n",
       " \"representin'\": 1,\n",
       " 'beck': 1,\n",
       " 'cornball': 1,\n",
       " 'expertly': 5,\n",
       " '1809': 1,\n",
       " 'boardman': 1,\n",
       " 'mija': 1,\n",
       " 'locataire': 3,\n",
       " 'undertaker': 1,\n",
       " 'warehouse': 1,\n",
       " 'rubble': 2,\n",
       " 'businessmen': 2,\n",
       " 'decision': 22,\n",
       " 'hamilton': 4,\n",
       " 'afternoon': 14,\n",
       " 'pathogen': 1,\n",
       " 'kyle': 5,\n",
       " 'ra': 3,\n",
       " 'busey': 3,\n",
       " 'tidbits': 1,\n",
       " 'regulatory': 1,\n",
       " 'ao': 1,\n",
       " 'weekly': 7,\n",
       " 'tod': 1,\n",
       " 'counterfeit': 1,\n",
       " 'knives': 1,\n",
       " 'tide': 3,\n",
       " 'strombel': 2,\n",
       " 'espouse': 1,\n",
       " \"'whoville'\": 1,\n",
       " 'sadly': 35,\n",
       " 'amir': 1,\n",
       " \"street'\": 1,\n",
       " 'buckle': 1,\n",
       " 'unsure': 3,\n",
       " 'fatty': 4,\n",
       " 'deception': 5,\n",
       " 'altar': 3,\n",
       " 'martial': 24,\n",
       " 'bars': 6,\n",
       " 'bluish': 1,\n",
       " 'arrondissement': 1,\n",
       " 'replace': 7,\n",
       " 'bedside': 1,\n",
       " 'blemished': 1,\n",
       " 'lust': 5,\n",
       " 'crouching': 1,\n",
       " 'blades': 1,\n",
       " 'winslet': 1,\n",
       " 'ness': 1,\n",
       " 'landscape': 4,\n",
       " 'incompatibility': 1,\n",
       " 'sept': 4,\n",
       " 'pipe': 1,\n",
       " 'business': 58,\n",
       " 'armando': 1,\n",
       " 'rosetti': 1,\n",
       " 'everyman': 1,\n",
       " 'grabbing': 1,\n",
       " 'picnic': 1,\n",
       " 'stage': 45,\n",
       " 'jnr': 1,\n",
       " 'lot': 274,\n",
       " 'hicks': 1,\n",
       " \"films'\": 3,\n",
       " 'voiced': 3,\n",
       " 'edgar': 8,\n",
       " 'overnight': 3,\n",
       " '20th': 6,\n",
       " 'allegories': 1,\n",
       " 'artistic': 38,\n",
       " 'entry': 14,\n",
       " 'frustrating': 7,\n",
       " 'probably': 224,\n",
       " 'lerner': 1,\n",
       " 'dumping': 1,\n",
       " 'oscars': 10,\n",
       " 'built': 19,\n",
       " 'arguably': 5,\n",
       " 'heretics': 1,\n",
       " 'bogie': 2,\n",
       " 'researched': 4,\n",
       " 'blaine': 3,\n",
       " 'tenacious': 3,\n",
       " 'tale': 54,\n",
       " 'austere': 2,\n",
       " 'zoo': 3,\n",
       " 'expedition': 4,\n",
       " 'mentalities': 1,\n",
       " 'quoted': 1,\n",
       " \"steel's\": 1,\n",
       " 'terminology': 1,\n",
       " 'dishes': 1,\n",
       " \"tykwer's\": 1,\n",
       " 'sneering': 1,\n",
       " 'wrestling': 5,\n",
       " 'yum': 1,\n",
       " 'small': 122,\n",
       " 'wear': 12,\n",
       " 'sikes': 4,\n",
       " 'cole': 12,\n",
       " 'baba': 2,\n",
       " 'jam': 1,\n",
       " '99': 4,\n",
       " 'kinski': 3,\n",
       " \"that'\": 2,\n",
       " 'abrasive': 2,\n",
       " 'fixing': 3,\n",
       " 'characterization': 14,\n",
       " 'parts': 101,\n",
       " 'papers': 3,\n",
       " 'pairs': 3,\n",
       " 'vancouver': 3,\n",
       " 'willing': 31,\n",
       " 'rainn': 1,\n",
       " 'happiest': 1,\n",
       " 'fagin': 3,\n",
       " 'releases': 6,\n",
       " 'blooded': 2,\n",
       " 'schoolteacher': 1,\n",
       " 'tormentors': 1,\n",
       " \"twin's\": 1,\n",
       " 'mechs': 2,\n",
       " 'unlikeable': 5,\n",
       " 'foxx': 14,\n",
       " 'joe': 60,\n",
       " 'trident': 1,\n",
       " 'mediterranean': 2,\n",
       " 'capra': 1,\n",
       " 'jokes': 86,\n",
       " 'filed': 1,\n",
       " 'paragraph': 2,\n",
       " \"'intensity'\": 1,\n",
       " 'major': 59,\n",
       " 'gourmet': 1,\n",
       " 'vigilantism': 1,\n",
       " 'subliminal': 2,\n",
       " \"forsythe's\": 1,\n",
       " 'control': 46,\n",
       " 'brundage': 2,\n",
       " 'tinkerbell': 1,\n",
       " 'reaching': 10,\n",
       " 'toss': 1,\n",
       " 'trench': 1,\n",
       " 'killing': 52,\n",
       " 'toy': 16,\n",
       " 'infested': 1,\n",
       " 'result': 47,\n",
       " 'resembling': 1,\n",
       " \"martha's\": 1,\n",
       " 'backstabbing': 2,\n",
       " 'reed’s': 1,\n",
       " 'fanatasies': 1,\n",
       " 'maniac': 7,\n",
       " 'stephens': 1,\n",
       " 'resembles': 11,\n",
       " 'magistral': 1,\n",
       " 'ttss': 1,\n",
       " 'decadent': 3,\n",
       " 'sexualised': 1,\n",
       " 'sensitive': 13,\n",
       " 'linda': 12,\n",
       " 'unimpressiveness': 1,\n",
       " 'eyeballs': 3,\n",
       " 'unless': 62,\n",
       " 'worsens': 1,\n",
       " 'transit': 1,\n",
       " 'britian': 1,\n",
       " 'boots': 2,\n",
       " 'airline': 2,\n",
       " 'vipco': 1,\n",
       " 'dazzler': 1,\n",
       " 'paces': 2,\n",
       " 'trueba': 1,\n",
       " 'liza': 3,\n",
       " 'attest': 1,\n",
       " 'doesn´t': 1,\n",
       " 'assassin': 10,\n",
       " 'deemed': 2,\n",
       " \"disc's\": 1,\n",
       " 'pals': 4,\n",
       " 'projectionist': 1,\n",
       " 'dean': 15,\n",
       " 'fabrications': 1,\n",
       " \"mcintire's\": 1,\n",
       " 'romps': 1,\n",
       " 'agreements': 2,\n",
       " 'crap': 79,\n",
       " '53': 2,\n",
       " 'nighttime': 3,\n",
       " 'dehumanizing': 1,\n",
       " 'hulce': 4,\n",
       " 'voters': 1,\n",
       " 'type': 81,\n",
       " 'attempting': 10,\n",
       " \"bond's\": 1,\n",
       " 'morbid': 7,\n",
       " 'perpetrators': 1,\n",
       " 'realistically': 4,\n",
       " 'expectation': 4,\n",
       " 'parody': 17,\n",
       " \"ferris's\": 2,\n",
       " 'parachute': 1,\n",
       " 'shower': 8,\n",
       " 'revisited': 2,\n",
       " 'plenty': 50,\n",
       " 'moreover': 9,\n",
       " 'graders': 1,\n",
       " 'ashton': 1,\n",
       " 'rosco': 1,\n",
       " 'dishing': 1,\n",
       " 'posit': 1,\n",
       " 'greenwood': 1,\n",
       " 'knotts': 1,\n",
       " 'offered': 13,\n",
       " 'introductions': 1,\n",
       " 'unbelievably': 3,\n",
       " 'straightheads': 5,\n",
       " 'freshness': 2,\n",
       " 'season': 56,\n",
       " \"grace's\": 1,\n",
       " 'unconvincingly': 1,\n",
       " 'coincidentally': 2,\n",
       " 'bootstraps': 1,\n",
       " 'marketplace': 1,\n",
       " \"'great'\": 1,\n",
       " 'says': 78,\n",
       " 'worldfest': 1,\n",
       " 'adding': 12,\n",
       " 'unheralded': 1,\n",
       " 'tired': 25,\n",
       " 'magazine': 13,\n",
       " 'timidly': 1,\n",
       " 'approaching': 3,\n",
       " 'risking': 2,\n",
       " 'imprisoned': 5,\n",
       " 'atlantic': 3,\n",
       " 'keir': 2,\n",
       " \"1940's\": 2,\n",
       " \"chang's\": 5,\n",
       " \"d'or\": 1,\n",
       " 'transforming': 2,\n",
       " 'scrambling': 1,\n",
       " 'pullman': 1,\n",
       " '68': 1,\n",
       " 'addicts': 1,\n",
       " 'lopez': 4,\n",
       " 'drops': 4,\n",
       " 'teenager': 11,\n",
       " 'hays': 1,\n",
       " 'dale': 4,\n",
       " 'impossibly': 5,\n",
       " 'filmmakes': 1,\n",
       " 'primarily': 3,\n",
       " 'denying': 3,\n",
       " 'tides': 3,\n",
       " \"delmar's\": 1,\n",
       " 'followers': 7,\n",
       " 'fratlike': 1,\n",
       " 'lollobrigida': 2,\n",
       " 'space': 53,\n",
       " 'statham': 1,\n",
       " 'christmave': 1,\n",
       " 'habituated': 1,\n",
       " 'kimmy': 1,\n",
       " 'chalk': 1,\n",
       " 'avenging': 1,\n",
       " 'hunted': 1,\n",
       " 'detroit': 5,\n",
       " 'smash': 5,\n",
       " 'tailored': 3,\n",
       " 'exact': 16,\n",
       " 'wikipedia': 2,\n",
       " 'input': 1,\n",
       " 'stronger': 13,\n",
       " 'reaction': 19,\n",
       " 'errand': 2,\n",
       " 'apotheosising': 1,\n",
       " 'provisions': 1,\n",
       " 'orson': 6,\n",
       " 'simulation': 1,\n",
       " 'absurdism': 1,\n",
       " 'condescendingly': 1,\n",
       " \"captain's\": 1,\n",
       " 'improves': 2,\n",
       " '3k': 1,\n",
       " 'sportswear': 1,\n",
       " 'futile': 2,\n",
       " 'passive': 4,\n",
       " 'wicker': 3,\n",
       " 'loach': 1,\n",
       " 'brad': 6,\n",
       " 'counterpoint': 6,\n",
       " 'slater': 11,\n",
       " 'mache': 1,\n",
       " \"anthony's\": 1,\n",
       " 'hitherto': 2,\n",
       " 'gant': 7,\n",
       " 'humane': 1,\n",
       " 'coburn': 1,\n",
       " 'patrons': 2,\n",
       " \"lilith's\": 1,\n",
       " 'turned': 80,\n",
       " 'ak': 1,\n",
       " \"friend's\": 1,\n",
       " \"ayers'\": 2,\n",
       " 'audibly': 1,\n",
       " 'ordinariness': 1,\n",
       " 'corrupt': 13,\n",
       " 'irritated': 4,\n",
       " 'enters': 9,\n",
       " 'nip': 1,\n",
       " 'rhonda': 1,\n",
       " \"'it'\": 1,\n",
       " \"iran's\": 2,\n",
       " 'mcgovernisms': 1,\n",
       " 'undermined': 1,\n",
       " 'jr': 21,\n",
       " 'contrary': 14,\n",
       " 'hint': 14,\n",
       " 'alpha': 3,\n",
       " \"god's\": 11,\n",
       " \"'your\": 1,\n",
       " \"smokin'\": 2,\n",
       " 'lacquered': 1,\n",
       " \"wellington's\": 2,\n",
       " 'organised': 1,\n",
       " 'reagan': 5,\n",
       " 'galileo': 1,\n",
       " 'opportunity': 27,\n",
       " 'lieu': 1,\n",
       " 'cruise': 4,\n",
       " 'greene': 7,\n",
       " \"script's\": 3,\n",
       " 'reverend': 1,\n",
       " 'smallest': 4,\n",
       " 'contrivance': 1,\n",
       " 'conduct': 2,\n",
       " 'faculty': 2,\n",
       " 'due': 71,\n",
       " 'protestant': 1,\n",
       " 'candidate': 6,\n",
       " \"antonioni's\": 1,\n",
       " '1989': 2,\n",
       " 'bloodedness': 1,\n",
       " 'sphincter': 1,\n",
       " 'ins': 2,\n",
       " 'hyped': 4,\n",
       " 'provokes': 1,\n",
       " 'blight': 1,\n",
       " 'sister': 63,\n",
       " 'rebellious': 8,\n",
       " 'karaoke': 1,\n",
       " 'melendez': 2,\n",
       " 'helping': 12,\n",
       " 'haired': 2,\n",
       " 'ilk': 2,\n",
       " 'sedative': 1,\n",
       " \"o'clock\": 1,\n",
       " 'cherish': 2,\n",
       " 'embarrass': 1,\n",
       " 'affords': 1,\n",
       " \"men's\": 7,\n",
       " \"'superthunderstingcar'\": 1,\n",
       " 'lowlifes': 2,\n",
       " 'summer': 26,\n",
       " 'site': 13,\n",
       " 'suffers': 15,\n",
       " \"block'\": 1,\n",
       " \"isn't\": 258,\n",
       " 'charts': 1,\n",
       " \"pressly's\": 1,\n",
       " 'transmits': 1,\n",
       " 'albert': 18,\n",
       " 'august': 1,\n",
       " 'inviolable': 1,\n",
       " 'judgments': 1,\n",
       " 'fews': 2,\n",
       " 'supported': 6,\n",
       " 'realises': 2,\n",
       " 'cognition': 2,\n",
       " 'pantheon': 1,\n",
       " 'widely': 8,\n",
       " 'okey': 1,\n",
       " 'nicholas': 16,\n",
       " 'fabric': 2,\n",
       " 'campiness': 1,\n",
       " 'overplay': 1,\n",
       " 'hail': 1,\n",
       " 'anchorman': 9,\n",
       " 'farnsworth': 2,\n",
       " 'enabling': 1,\n",
       " 'structuralist': 1,\n",
       " 'existence': 19,\n",
       " 'cow': 1,\n",
       " 'shaky': 5,\n",
       " \"man's\": 24,\n",
       " 'comes': 189,\n",
       " 'dispiriting': 1,\n",
       " 'joburg': 1,\n",
       " 'lamer': 1,\n",
       " 'drift': 2,\n",
       " \"harpo's\": 1,\n",
       " 'footage': 37,\n",
       " 'bentley': 2,\n",
       " 'feasts': 1,\n",
       " 'testosterone': 3,\n",
       " 'twisty': 1,\n",
       " 'lambada': 1,\n",
       " 'pharmacy': 1,\n",
       " 'forgetting': 7,\n",
       " 'reworks': 1,\n",
       " 'grubbing': 1,\n",
       " 'factual': 2,\n",
       " 'simply': 126,\n",
       " 'activities': 9,\n",
       " 'chrstian': 1,\n",
       " 'fantafestival': 1,\n",
       " 'admitting': 2,\n",
       " 'unjustified': 1,\n",
       " 'pocket': 6,\n",
       " 'noodle': 1,\n",
       " \"lead's\": 1,\n",
       " 'shimada': 1,\n",
       " 'ascending': 1,\n",
       " 'manchuria': 1,\n",
       " 'potentially': 5,\n",
       " 'electrifying': 4,\n",
       " 'hardness': 1,\n",
       " 'tiring': 1,\n",
       " '71': 3,\n",
       " 'worships': 1,\n",
       " 'coat': 3,\n",
       " 'students': 35,\n",
       " 'fathers': 7,\n",
       " 'answer': 22,\n",
       " 'nephews': 1,\n",
       " 'garish': 1,\n",
       " 'conflict': 30,\n",
       " 'church': 36,\n",
       " 'hannah': 3,\n",
       " 'drake': 8,\n",
       " 'scramble': 1,\n",
       " 'lettieri': 2,\n",
       " 'teachers': 6,\n",
       " 'stairs': 4,\n",
       " 'gathers': 1,\n",
       " 'sharukh': 1,\n",
       " '1948': 2,\n",
       " 'fruits': 2,\n",
       " 'dey': 1,\n",
       " 'bilodeau': 1,\n",
       " 'pose': 5,\n",
       " \"ruby's\": 1,\n",
       " \"latter's\": 1,\n",
       " 'guerrilla': 1,\n",
       " 'closest': 12,\n",
       " 'spot': 31,\n",
       " 'spellbound': 2,\n",
       " 'bashing': 5,\n",
       " 'scriptwriter': 3,\n",
       " 'philippe': 9,\n",
       " 'fury': 5,\n",
       " 'marque': 1,\n",
       " 'boheme': 1,\n",
       " 'several': 121,\n",
       " 'constructed': 9,\n",
       " 'understands': 3,\n",
       " 'permanent': 4,\n",
       " 'family': 223,\n",
       " 'showcase': 6,\n",
       " 'outstripping': 1,\n",
       " 'astonishingly': 2,\n",
       " 'pathos': 4,\n",
       " 'adopting': 2,\n",
       " 'footnote': 1,\n",
       " \"1953's\": 1,\n",
       " 'found': 193,\n",
       " 'sanchez': 2,\n",
       " \"vidal's\": 1,\n",
       " \"'sci\": 1,\n",
       " 'vices': 1,\n",
       " 'sunnier': 1,\n",
       " 'reign': 3,\n",
       " 'samoa': 1,\n",
       " 'blast': 9,\n",
       " 'bonded': 1,\n",
       " 'à': 2,\n",
       " '70s': 16,\n",
       " 'feeble': 4,\n",
       " 'contemporary': 10,\n",
       " 'antagonism': 1,\n",
       " 'horribleness': 1,\n",
       " 'recapture': 5,\n",
       " 'rejected': 3,\n",
       " 'depart': 2,\n",
       " 'pakistani': 1,\n",
       " \"'deep\": 1,\n",
       " 'bully': 1,\n",
       " 'electrician': 1,\n",
       " 'pokes': 3,\n",
       " 'observe': 4,\n",
       " \"clive's\": 1,\n",
       " 'centre': 8,\n",
       " 'lonely': 17,\n",
       " 'failures': 4,\n",
       " 'burbank': 2,\n",
       " 'set': 203,\n",
       " 'push': 8,\n",
       " '47s': 1,\n",
       " 'clemens': 3,\n",
       " 'yorga': 1,\n",
       " 'subservient': 1,\n",
       " 'snooze': 1,\n",
       " 'invites': 8,\n",
       " 'uncomfortableness': 1,\n",
       " 'bank': 14,\n",
       " 'manifests': 2,\n",
       " 'fitting': 12,\n",
       " 'jarring': 6,\n",
       " 'gielgud': 4,\n",
       " 'sermon': 1,\n",
       " 'rainbow': 5,\n",
       " 'cavemen': 5,\n",
       " 'capes': 1,\n",
       " 'rao': 1,\n",
       " \"giordano's\": 1,\n",
       " 'disarray': 1,\n",
       " 'sufficient': 6,\n",
       " 'long': 257,\n",
       " 'pianists': 2,\n",
       " 'training': 11,\n",
       " 'knack': 2,\n",
       " 'scenes': 379,\n",
       " 'proletarian': 1,\n",
       " 'odious': 1,\n",
       " 'sham': 4,\n",
       " 'jess': 11,\n",
       " 'gazzara': 3,\n",
       " 'sanxia': 1,\n",
       " 'replay': 3,\n",
       " 'inherent': 4,\n",
       " 'gauche': 1,\n",
       " 'miramax': 8,\n",
       " 'flimsy': 4,\n",
       " 'shipmates': 1,\n",
       " \"homer's\": 5,\n",
       " 'junk': 15,\n",
       " 'slipped': 1,\n",
       " 'maoris': 1,\n",
       " 'admit': 51,\n",
       " 'championship': 1,\n",
       " 'possess': 5,\n",
       " 'lecherous': 1,\n",
       " 'triumphs': 3,\n",
       " 'sixty': 1,\n",
       " 'lanza': 2,\n",
       " 'tastefully': 2,\n",
       " \"promo's\": 1,\n",
       " 'encroachment': 1,\n",
       " 'alternates': 2,\n",
       " 'freleng': 1,\n",
       " \"1950's\": 13,\n",
       " \"j's\": 1,\n",
       " 'continuity': 12,\n",
       " 'eagerly': 2,\n",
       " 'suffer': 15,\n",
       " 'shocks': 5,\n",
       " 'moses': 3,\n",
       " 'thinks': 28,\n",
       " 'scary': 75,\n",
       " 'ok': 60,\n",
       " 'persuading': 1,\n",
       " 'lunch': 2,\n",
       " 'fund': 1,\n",
       " 'relative': 8,\n",
       " 'outrageous': 13,\n",
       " ...}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have trained the tokenizer we can use it to vectorize the texts. In particular, we would like to transform the texts to sequences of word indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Find in the keras documentation the appropriate Tokenizer method to transform a list of texts to a sequence. Apply it to both the training and test data to obtain matrices **X_train** and **X_test**.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1875\n",
      "625\n",
      "157 167 98\n"
     ]
    }
   ],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "X_train = tokenizer.texts_to_sequences(texts_train)\n",
    "X_test = tokenizer.texts_to_sequences(texts_test)\n",
    "print(type(X_train))\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(X_train[0]),len(X_train[1]),len(X_train[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9,\n",
       " 280,\n",
       " 2,\n",
       " 52,\n",
       " 323,\n",
       " 273,\n",
       " 4,\n",
       " 438,\n",
       " 5,\n",
       " 86,\n",
       " 2,\n",
       " 18,\n",
       " 12,\n",
       " 6,\n",
       " 37,\n",
       " 3,\n",
       " 17,\n",
       " 86,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 92,\n",
       " 12,\n",
       " 75,\n",
       " 167,\n",
       " 9,\n",
       " 490,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 29,\n",
       " 4,\n",
       " 24,\n",
       " 39,\n",
       " 46,\n",
       " 3,\n",
       " 329,\n",
       " 91,\n",
       " 142,\n",
       " 12,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 183,\n",
       " 12,\n",
       " 42,\n",
       " 456,\n",
       " 11,\n",
       " 19,\n",
       " 197,\n",
       " 5,\n",
       " 1,\n",
       " 475,\n",
       " 4,\n",
       " 3,\n",
       " 400,\n",
       " 3,\n",
       " 290,\n",
       " 7,\n",
       " 7,\n",
       " 192,\n",
       " 119,\n",
       " 1,\n",
       " 1,\n",
       " 19,\n",
       " 280,\n",
       " 20,\n",
       " 2,\n",
       " 265,\n",
       " 45,\n",
       " 46,\n",
       " 62,\n",
       " 6,\n",
       " 2,\n",
       " 31,\n",
       " 29,\n",
       " 3,\n",
       " 9,\n",
       " 16,\n",
       " 1,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 336,\n",
       " 2,\n",
       " 128,\n",
       " 16,\n",
       " 44,\n",
       " 265,\n",
       " 537,\n",
       " 17,\n",
       " 45,\n",
       " 21,\n",
       " 66,\n",
       " 1,\n",
       " 15,\n",
       " 1,\n",
       " 2,\n",
       " 95,\n",
       " 1,\n",
       " 9,\n",
       " 632,\n",
       " 53,\n",
       " 2,\n",
       " 4,\n",
       " 12,\n",
       " 924,\n",
       " 21,\n",
       " 745,\n",
       " 11,\n",
       " 6,\n",
       " 400,\n",
       " 38,\n",
       " 43,\n",
       " 1,\n",
       " 8,\n",
       " 324,\n",
       " 21,\n",
       " 263,\n",
       " 1,\n",
       " 12,\n",
       " 11,\n",
       " 19,\n",
       " 81,\n",
       " 978,\n",
       " 9,\n",
       " 81,\n",
       " 102,\n",
       " 26,\n",
       " 23,\n",
       " 55,\n",
       " 30,\n",
       " 1,\n",
       " 419,\n",
       " 4,\n",
       " 1,\n",
       " 183,\n",
       " 34,\n",
       " 955,\n",
       " 41,\n",
       " 1,\n",
       " 389,\n",
       " 21,\n",
       " 213,\n",
       " 23,\n",
       " 35,\n",
       " 9,\n",
       " 17,\n",
       " 9,\n",
       " 6,\n",
       " 429,\n",
       " 142,\n",
       " 255,\n",
       " 140]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 280, 2, 52, 323, 273, 4, 438, 5, 86, 2, 18, 12, 6, 37, 3, 17, 86, 9, 8, 2, 92, 12, 75, 167, 9, 490, 3, 1, 4, 8, 29, 4, 24, 39, 46, 3, 329, 91, 142, 12, 6, 8, 2, 183, 12, 42, 456, 11, 19, 197, 5, 1, 475, 4, 3, 400, 3, 290, 7, 7, 192, 119, 1, 1, 19, 280, 20, 2, 265, 45, 46, 62, 6, 2, 31, 29, 3, 9, 16, 1, 4, 8, 4, 2, 3, 336, 2, 128, 16, 44, 265, 537, 17, 45, 21, 66, 1, 15, 1, 2, 95, 1, 9, 632, 53, 2, 4, 12, 924, 21, 745, 11, 6, 400, 38, 43, 1, 8, 324, 21, 263, 1, 12, 11, 19, 81, 978, 9, 81, 102, 26, 23, 55, 30, 1, 419, 4, 1, 183, 34, 955, 41, 1, 389, 21, 213, 23, 35, 9, 17, 9, 6, 429, 142, 255, 140]\n",
      "[10, 11, 107, 32, 51, 10, 13, 2, 694, 10, 518, 332, 96, 71, 41, 9, 43, 2, 152, 159, 41, 1, 98, 15, 48, 262, 10, 9, 108, 62, 75, 9, 13, 20, 62, 62, 352, 8, 1, 53, 8, 10, 368, 95, 266, 165, 1, 15, 2, 202, 56, 259, 33, 359, 1, 78, 359, 59, 334, 11, 107, 13, 37, 565, 9, 13, 506, 43, 91, 5, 107, 351, 404, 3, 256, 2, 80, 56, 1, 107, 16, 929, 24, 54, 23, 139, 72, 99, 22, 976, 5, 112, 351, 404, 5, 1, 609, 3, 23, 43, 2, 122, 222, 17, 1, 230, 609, 54, 43, 529, 41, 9, 483, 3, 14, 45, 10, 518, 73, 383, 21, 55, 61, 1, 32, 68, 88, 35, 463, 43, 576, 165, 5, 1, 424, 609, 257, 20, 95, 264, 53, 84, 882, 38, 113, 129, 36, 27, 2, 485, 5, 1, 609, 69, 21, 90, 205, 5, 116, 1, 362, 47, 65, 10, 136, 1, 610]\n",
      "[11, 6, 28, 4, 1, 78, 3, 115, 587, 3, 587, 20, 163, 14, 121, 409, 76, 495, 3, 75, 91, 2, 257, 7, 7, 8, 11, 28, 6, 24, 282, 375, 258, 30, 6, 256, 33, 37, 25, 5, 42, 2, 819, 20, 1, 395, 37, 25, 420, 2, 3, 16, 1, 149, 28, 45, 57, 138, 945, 53, 36, 73, 84, 2, 619, 485, 8, 350, 3, 57, 53, 7, 7, 123, 287, 25, 65, 5, 73, 119, 1, 432, 495, 93, 2, 300, 17, 6, 20, 5, 93, 32, 1, 491, 7, 7, 357, 359]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(X_train[elemento]) for elemento in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is enough to train a Sequential Network. However, for efficiency reasons it is recommended that all sequences in the data have the same number of elements. Since this is not the case for our data, should **pad** the sequences to ensure the same length. The padding procedure adds a special *null* symbol to short sequences, and clips out parts of long sequences, thus enforcing a common size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Find in the keras documentation the appropriate text preprocessing method to pad a sequence. Then pad all sequences to have a maximum of 300 words, both in the training and test data.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "1875\n",
      "625\n",
      "300 300 300\n"
     ]
    }
   ],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "from keras.preprocessing import sequence\n",
    "max_length = 300\n",
    "X_train_pad = sequence.pad_sequences(X_train, maxlen=max_length)\n",
    "X_test_pad = sequence.pad_sequences(X_test, maxlen=max_length)\n",
    "\n",
    "print(type(X_train_pad))\n",
    "print(len(X_train_pad))\n",
    "print(len(X_test_pad))\n",
    "print(len(X_train_pad[i]),len(X_train_pad[1]),len(X_train_pad[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   9\n",
      " 280   2  52 323 273   4 438   5  86   2  18  12   6  37   3  17  86   9\n",
      "   8   2  92  12  75 167   9 490   3   1   4   8  29   4  24  39  46   3\n",
      " 329  91 142  12   6   8   2 183  12  42 456  11  19 197   5   1 475   4\n",
      "   3 400   3 290   7   7 192 119   1   1  19 280  20   2 265  45  46  62\n",
      "   6   2  31  29   3   9  16   1   4   8   4   2   3 336   2 128  16  44\n",
      " 265 537  17  45  21  66   1  15   1   2  95   1   9 632  53   2   4  12\n",
      " 924  21 745  11   6 400  38  43   1   8 324  21 263   1  12  11  19  81\n",
      " 978   9  81 102  26  23  55  30   1 419   4   1 183  34 955  41   1 389\n",
      "  21 213  23  35   9  17   9   6 429 142 255 140]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0  10  11 107  32  51  10  13   2 694  10 518\n",
      " 332  96  71  41   9  43   2 152 159  41   1  98  15  48 262  10   9 108\n",
      "  62  75   9  13  20  62  62 352   8   1  53   8  10 368  95 266 165   1\n",
      "  15   2 202  56 259  33 359   1  78 359  59 334  11 107  13  37 565   9\n",
      "  13 506  43  91   5 107 351 404   3 256   2  80  56   1 107  16 929  24\n",
      "  54  23 139  72  99  22 976   5 112 351 404   5   1 609   3  23  43   2\n",
      " 122 222  17   1 230 609  54  43 529  41   9 483   3  14  45  10 518  73\n",
      " 383  21  55  61   1  32  68  88  35 463  43 576 165   5   1 424 609 257\n",
      "  20  95 264  53  84 882  38 113 129  36  27   2 485   5   1 609  69  21\n",
      "  90 205   5 116   1 362  47  65  10 136   1 610]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0  11   6  28   4   1  78   3 115 587   3 587  20 163  14\n",
      " 121 409  76 495   3  75  91   2 257   7   7   8  11  28   6  24 282 375\n",
      " 258  30   6 256  33  37  25   5  42   2 819  20   1 395  37  25 420   2\n",
      "   3  16   1 149  28  45  57 138 945  53  36  73  84   2 619 485   8 350\n",
      "   3  57  53   7   7 123 287  25  65   5  73 119   1 432 495  93   2 300\n",
      "  17   6  20   5  93  32   1 491   7   7 357 359]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(X_train_pad[elemento]) for elemento in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure indexes model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first try to build a model based just on word indexes. Since keras expects sequential inputs as 3-dimensional array with dimensions NUMBER_SEQUENCES x SEQUENCE_LENGTH x FEATURES and we will we using only the indexes, our features dimension is one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Create new variables **X_train_idx** and **X_test_idx**, reshaped versions of **X_train** and **X_test**, in which each index has been transformed into a 1-element list.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "X_train_idx =X_train_pad.reshape(1875,300,1)\n",
    "X_test_idx = X_test_pad.reshape(625,300,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Build, compile and train a keras network with an LSTM layer of 32 units and dropout 0.9, followed by a Dense layer of 1 unit with sigmoid activation. Use the binary crossentroy loss function for training, together with the adam optimizer. Train for 10 epochs. After training, measure the accuracy on the test set.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "\n",
    "modelo = Sequential()\n",
    "modelo.add(LSTM(32,input_shape =(300,1)))\n",
    "modelo.add(Dropout(0.9))\n",
    "modelo.add(Dense(1))\n",
    "modelo.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_1 (LSTM)                    (None, 32)            4352        lstm_input_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 32)            0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             33          dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 1)             0           dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 4,385\n",
      "Trainable params: 4,385\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelo.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "54s - loss: 0.9978 - acc: 0.4832\n",
      "Epoch 2/10\n",
      "47s - loss: 0.9294 - acc: 0.5083\n",
      "Epoch 3/10\n",
      "49s - loss: 0.9189 - acc: 0.4795\n",
      "Epoch 4/10\n",
      "47s - loss: 0.8506 - acc: 0.4965\n",
      "Epoch 5/10\n",
      "47s - loss: 0.8273 - acc: 0.5195\n",
      "Epoch 6/10\n",
      "46s - loss: 0.8005 - acc: 0.5131\n",
      "Epoch 7/10\n",
      "46s - loss: 0.7451 - acc: 0.4896\n",
      "Epoch 8/10\n",
      "47s - loss: 0.7393 - acc: 0.5061\n",
      "Epoch 9/10\n",
      "46s - loss: 0.7386 - acc: 0.4976\n",
      "Epoch 10/10\n",
      "46s - loss: 0.7245 - acc: 0.4981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1042e780>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.fit(\n",
    "    X_train_idx,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    nb_epoch=10,\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 6s     \n",
      "\n",
      "Test loss 0.692267370224\n",
      "Test accuracy 0.505600000572\n"
     ]
    }
   ],
   "source": [
    "score = modelo.evaluate(X_test_idx, y_test)\n",
    "print(\"\")\n",
    "print(\"Test loss\", score[0])\n",
    "print(\"Test accuracy\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning an embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using indexes as a representation of words is a very poor approach. We can easily improve over that by using an **Embedding** layer at the very beginning of the network. This layer will transform word indexes to a vector representation that is learned with the model together with the rest of network weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Create a new network similar to the previous one, but adding an Embedding as the first layer of the network. Configure the Embedding layer to produce a vector representation of 64 elements. Then train the network with similar setting to the previous one. Has the test accuracy improved?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/exclamation.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "The Embedding layer accepts lists of indexes as inputs, so you don't need to use the **X_train_idx** representation you created for the previous network.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 300, 64)       64000       embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 32)            12416       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 32)            0           lstm_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             33          dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 1)             0           dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 76,449\n",
      "Trainable params: 76,449\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "from keras.layers.embeddings import Embedding\n",
    "embedding_vector =64\n",
    "\n",
    "model_emb = Sequential()\n",
    "\n",
    "model_emb.add(Embedding(maxwords,embedding_vector, input_length=max_length))\n",
    "model_emb.add(LSTM(32))\n",
    "model_emb.add(Dropout(0.9))\n",
    "model_emb.add(Dense(1))\n",
    "model_emb.add(Activation('sigmoid'))\n",
    "\n",
    "model_emb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "66s - loss: 0.6936 - acc: 0.4965\n",
      "Epoch 2/10\n",
      "59s - loss: 0.6832 - acc: 0.5595\n",
      "Epoch 3/10\n",
      "56s - loss: 0.6493 - acc: 0.6469\n",
      "Epoch 4/10\n",
      "55s - loss: 0.5514 - acc: 0.7541\n",
      "Epoch 5/10\n",
      "57s - loss: 0.4923 - acc: 0.7925\n",
      "Epoch 6/10\n",
      "57s - loss: 0.4679 - acc: 0.8171\n",
      "Epoch 7/10\n",
      "57s - loss: 0.4398 - acc: 0.8501\n",
      "Epoch 8/10\n",
      "56s - loss: 0.3817 - acc: 0.8571\n",
      "Epoch 9/10\n",
      "56s - loss: 0.3627 - acc: 0.8715\n",
      "Epoch 10/10\n",
      "56s - loss: 0.3327 - acc: 0.8864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12c57a58>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_emb.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "model_emb.fit(\n",
    "    X_train_pad,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    nb_epoch=10,\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 7s     \n",
      "\n",
      "Test loss 0.482038615704\n",
      "Test accuracy 0.777600000286\n"
     ]
    }
   ],
   "source": [
    "score_emb = model_emb.evaluate(X_test_pad, y_test)\n",
    "print(\"\")\n",
    "print(\"Test loss\", score_emb[0])\n",
    "print(\"Test accuracy\", score_emb[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">La *accuracy* en test mejora muchísimo con la capa de embedding.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like other neural layers, LSTM layers can be stacked on top of each other to produce more complex models. Care must be taken, however, that the LSTM layers before the last one generate a whole sequence of outputs for the following LSTM to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td><img src=\"img/question.png\" style=\"width:80px;height:80px;\"></td><td>\n",
    "Repeat the training of the previous network, but using 2 LSTM layers. Make sure to configure the first LSTM layer in a way that it outputs a whole sequence for the next layer.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 300, 64)       64000       embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    (None, 300, 32)       12416       embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 300, 32)       0           lstm_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 32)            8320        dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 32)            0           lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             33          dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 1)             0           dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 84,769\n",
      "Trainable params: 84,769\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "modelo_2lstm = Sequential()\n",
    "\n",
    "modelo_2lstm.add(Embedding(maxwords,embedding_vector, input_length=max_length))\n",
    "modelo_2lstm.add(LSTM(32, return_sequences =True))\n",
    "modelo_2lstm.add(Dropout(0.9))\n",
    "modelo_2lstm.add(LSTM(32))\n",
    "modelo_2lstm.add(Dropout(0.9))\n",
    "modelo_2lstm.add(Dense(1))\n",
    "modelo_2lstm.add(Activation('sigmoid'))\n",
    "\n",
    "modelo_2lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "128s - loss: 0.6962 - acc: 0.5003\n",
      "Epoch 2/10\n",
      "111s - loss: 0.6923 - acc: 0.5349\n",
      "Epoch 3/10\n",
      "110s - loss: 0.6862 - acc: 0.5381\n",
      "Epoch 4/10\n",
      "113s - loss: 0.6583 - acc: 0.5973\n",
      "Epoch 5/10\n",
      "111s - loss: 0.5715 - acc: 0.7211\n",
      "Epoch 6/10\n",
      "111s - loss: 0.4800 - acc: 0.7904\n",
      "Epoch 7/10\n",
      "111s - loss: 0.4210 - acc: 0.8373\n",
      "Epoch 8/10\n",
      "116s - loss: 0.3644 - acc: 0.8683\n",
      "Epoch 9/10\n",
      "112s - loss: 0.3107 - acc: 0.8949\n",
      "Epoch 10/10\n",
      "113s - loss: 0.2883 - acc: 0.9104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b659390>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_2lstm.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "modelo_2lstm.fit(\n",
    "    X_train_pad,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    nb_epoch=10,\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 18s    \n",
      "\n",
      "Test loss 0.722634818172\n",
      "Test accuracy 0.724800000668\n"
     ]
    }
   ],
   "source": [
    "score_2lstm = modelo_2lstm.evaluate(X_test_pad, y_test)\n",
    "print(\"\")\n",
    "print(\"Test loss\", score_2lstm[0])\n",
    "print(\"Test accuracy\", score_2lstm[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">La *accuracy* en test no mejora al añadir otra capa. De hecho, empeora sensiblemente.</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
